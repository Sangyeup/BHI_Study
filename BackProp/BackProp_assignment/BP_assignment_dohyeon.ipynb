{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training with Backpropagation\n",
    "\n",
    "- 막히는 부분 있으면, https://github.com/WegraLee/deep-learning-from-scratch/tree/master/ch05 참고!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 상위 경로에 있는 functions, gradient 등을 import할 수 있게\n",
    "\n",
    "import numpy as np\n",
    "from functions import sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define layers as class\n",
    "\n",
    "* All layers have three methods\n",
    "    - `__init__(self, ...)`\n",
    "    - `forward(self, x)`\n",
    "    - `backward(self, dout)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Activation layers\n",
    "- ReLU\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "\n",
    "<img src=\"https://i.imgur.com/FrxDrr5.png\" width=\"600\">\n",
    "\n",
    "<figcaption align=\"center\"> - Backpropagation of relu node\n",
    "</figcaption>\n",
    "    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None  # input에서 어떤 원소가 0 이하인지 저장하는 mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### Type your code below ###\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        ### Type your code below ###\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "\n",
    "<img src=\"https://i.imgur.com/riURjqG.png\" width=\"500\">\n",
    "\n",
    "<figcaption align=\"center\"> - Backpropagation of sigmoid node\n",
    "</figcaption>\n",
    "    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### Type your code below ###\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        ### Type your code below ###\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Affine layer\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F994510365B98F75122F136\" width=\"600\">\n",
    "\n",
    "<figcaption align=\"center\"> - Backpropagation of affine layer\n",
    "</figcaption>\n",
    "    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Type your code below ###\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # 역전파가 이어져야 할 것은 input X에 대한 미분값뿐. (X = 이전 layer의 output이므로)\n",
    "        # W와 b에 대한 미분값은 이 layer의 W와 b를 업데이트할 때 쓰기 위해 이 layer에다 저장해놓으면 됨.\n",
    "        \n",
    "        ### Type your code below ###\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Softmax-with-Loss layer\n",
    "\n",
    "<figure>\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrWMeM%2FbtqQptySbcy%2FOcmx41ncd8SD6e7nPhVAkK%2Fimg.png\" width=\"600\">\n",
    "\n",
    "<figcaption align=\"center\"> - Backpropagation of softmax-with-loss layer\n",
    "</figcaption>\n",
    "    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import softmax, cross_entropy_error  # 지난번에 이미 구현했으므로 import해서 씁시다\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None  # softmax의 출력 (확률 벡터)\n",
    "        self.t = None  # 정답 label (one-hot vector)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        ### Type your code below ###\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        # 이 layer의 output이 최종 loss이므로, 상류에서 오는 미분값은 항상 1\n",
    "        \n",
    "        batch_size = self.t.shape[0]  # 구한 dx를 batch_size로 나누어 sample 1개당 오차를 앞 계층으로 전파하는 것 주의!\n",
    "        \n",
    "        ### Type your code below ###\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient import numerical_gradient  # 지난주에 구현했으니 import해서 사용\n",
    "from collections import OrderedDict  # layer들을 순서대로 저장하기 위해 사용\n",
    "\n",
    "class AnyLayerNet:\n",
    "    \"\"\"\n",
    "    원하는 만큼 layer를 추가하여 구현해 보세요.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "        # Initialize parameters\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        ### Type your code below ###\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # Create layers\n",
    "        self.layers = OrderedDict()  # 순서를 기억하는 dictionary\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        #self.layers['Relu2'] = ReLU()\n",
    "        ### Type your code below ###\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        ### Type your code below ###\n",
    "        # Hint: SoftmaxWithLoss layer는 여기서 call하지 않음. (prediction 시에는 logit 값만 내놓으면 되니 그런 듯)\n",
    "        # self.layers에 담긴 layer들의 forward만 call할 것.\n",
    "        \n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        # x: 입력 데이터, t: 정답 label\n",
    "        \n",
    "        ### Type your code below ###\n",
    "        # Hint: self.predict에서 lastLayer는 call하지 않았으므로, 여기서 따로 call해줄 것.\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        ### Type your code below ###\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis = 1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        # 저번주에 구현했으니 그대로 씁시다\n",
    "        \"\"\"다만, Layer를 2층보다 더 추가했다면 아래 code에도 추가해주어야 함!\"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        ### Type your code below ###\n",
    "        \n",
    "        # 1. Forward path (calculate loss)\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 2. Backpropagation\n",
    "        # Hint: self.layers를 list로 만들어 순서를 뒤집은 뒤, for loop\n",
    "\n",
    "        dout = 1  # dL/dL\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # grads dictionary에 gradient 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare numerical gradient & Backpropagation\n",
    "- 배치 하나로 gradient를 구하는데 각각 얼마나 걸리는지 비교해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import load_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, \n",
    "                                                  flatten=True, \n",
    "                                                  one_hot_label=True)\n",
    "\n",
    "X_batch = X_train[:3]\n",
    "y_batch = y_train[:3]\n",
    "\n",
    "# print(X_batch.shape)\n",
    "# print(y_batch.shape)\n",
    "### Type your code below ###\n",
    "network = AnyLayerNet(input_size = 784, hidden_size = 100, output_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 2.49 s, total: 1min 12s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get gradient with numerical_gradient\n",
    "\n",
    "### Type your code below ###\n",
    "grad_numerical = network.numerical_gradient(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 16.2 ms, total: 16.2 ms\n",
      "Wall time: 2.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get gradient with backrpopagation\n",
    "\n",
    "### Type your code below ###\n",
    "grad_backprop = network.gradient(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Network\n",
    "    - Backpropagation을 통해 모델을 트레이닝하고,\n",
    "    - Epoch별 Train accuracy, Test accuracy를 plotting해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "0.10106666666666667 0.1\n",
      "0.9033666666666667 0.9063\n",
      "0.9265833333333333 0.9287\n",
      "0.9389666666666666 0.9374\n",
      "0.9489 0.9469\n",
      "0.956 0.9532\n",
      "0.9602166666666667 0.9574\n",
      "0.9655333333333334 0.9613\n",
      "0.9687333333333333 0.9649\n",
      "0.97165 0.9674\n",
      "0.9751666666666666 0.9695\n",
      "0.9768666666666667 0.9704\n",
      "0.9786666666666667 0.9713\n",
      "0.9803 0.9731\n",
      "0.9813166666666666 0.9733\n",
      "0.9829333333333333 0.9736\n",
      "0.9835333333333334 0.9734\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True,\n",
    "                                                  flatten=True,\n",
    "                                                  one_hot_label=True)\n",
    "\n",
    "\n",
    "### Type your code below ###\n",
    "network = AnyLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = X_train.shape[0]\n",
    "print(train_size)\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(X_batch, y_batch)\n",
    "    \n",
    "    # 파라미터 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # Loss 계산하여 list에 추가\n",
    "    loss = network.loss(X_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        # 매 epoch마다, accuracy를 계산하여 list에 추가하고 print\n",
    "        train_acc = network.accuracy(X_train, y_train)\n",
    "        test_acc = network.accuracy(X_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print (train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Type your code below ###\n",
    "\n",
    "x = np.arange(len(train_acc_list))\n",
    "\n",
    "plt.plot(x, train_acc_list, label = 'train_acc')\n",
    "plt.plot(x, test_acc_list, label = 'test_acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
