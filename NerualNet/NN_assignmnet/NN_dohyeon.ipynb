{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97571e35",
   "metadata": {},
   "source": [
    "# Build 3 Layer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f07ec",
   "metadata": {},
   "source": [
    "- input layer\n",
    "- 2 hidden layer\n",
    "- output layer\n",
    "<br>\n",
    "<br>\n",
    "- Loss function : Cross Entropy Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4140c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bddbfd",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbc4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c032019c-f703-465f-9b89-1d58071150ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 5, 6, 3],\n",
    "             [3, 5, 62, 1, 2],\n",
    "             [2, 3, 4, 5, 10]])\n",
    "a.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca860c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy_error\n",
    "# y : predicted value\n",
    "# t : label\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \n",
    "    # y: 신경망 출력\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    # t: 숫자 레이블인 경우\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    \n",
    "    return  -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145cea21-88bf-4134-8920-c4eacfe23375",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 5, 6, 3],\n",
    "             [3, 5, 62, 1, 2],\n",
    "             [2, 3, 4, 5, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65e5ccc-18d6-4922-acd9-9bbc12de341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = np.max(a, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa62e1cd-e2f1-4275-8d2d-e412483edb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.max(a.T, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c5e064-a345-4d65-bc62-bf16e2ce79b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5, -59,  -8],\n",
       "       [ -4, -57,  -7],\n",
       "       [ -1,   0,  -6],\n",
       "       [  0, -61,  -5],\n",
       "       [ -3, -60,   0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T - nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50dbc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "# x : input\n",
    "# Overflow 방지 -> input 중 가장 큰 값 전체 데이터에서 빼주기\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    if x.ndim == 2:\n",
    "        #Broadcasting을 하기 위해\n",
    "        x = x.T - np.max(x, axis = 1)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "        return y.T\n",
    "        \n",
    "    c = np.max(x)\n",
    "    x = x - c    \n",
    "    return  np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8442e575-cbcf-4b6f-8a12-ef812f20ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "(3,)\n",
      "[3 4 5]\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "x = [[1, 2, 3],\n",
    "    [3, 4, 5]]\n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "for idx in range(x.shape[0]):\n",
    "    print(x[idx])\n",
    "    print(x[idx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c8f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_gradient\n",
    "# 수치 미분법\n",
    "# f : function\n",
    "# x : variable\n",
    "# 각 변수에 대한 편미분 저장된 배열 return\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    \n",
    "    iter = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    with iter:\n",
    "        while not iter.finished:\n",
    "            idx = iter.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x)\n",
    "            \n",
    "            x[idx] = float(tmp_val) - h\n",
    "            fxh2 = f(x)\n",
    "            \n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "            \n",
    "            x[idx] = tmp_val\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c9d0c",
   "metadata": {},
   "source": [
    "## Define Network by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad56126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet:\n",
    "    # 초기화\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        # initialize parameters\n",
    "        # W -> random \n",
    "        # b -> 0\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size_1)\n",
    "        self.params['b1'] = np.zeros(hidden_size_1)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size_1, hidden_size_2)\n",
    "        self.params['b2'] = np.zeros(hidden_size_2)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size_2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "    # 예측 함수\n",
    "    # x : 입력 데이터\n",
    "    # y : 예측값\n",
    "    def predict(self,x):\n",
    "        \n",
    "        #### 채워 넣으세요 ####\n",
    "        # activation function : output 이전은 sigmoid\n",
    "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = sigmoid(a2)\n",
    "        a3 = np.dot(z2, W3) + b3\n",
    "        y = softmax(a3)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    # 손실 함수\n",
    "    # cross_entropy_error 사용\n",
    "    # x : 입력 데이터\n",
    "    # t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        \n",
    "        #### 채워 넣으세요 ####\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return  cross_entropy_error(y, t)\n",
    "    \n",
    "    \n",
    "    # 정확도 함수\n",
    "    # x : 입력 데이터\n",
    "    # t : 정답 레이블\n",
    "    # x 중 예측값 == 정답 레이블 비율\n",
    "    # accuracy = (정답 맞춘 데이터 수) / (전체 데이터 수)\n",
    "    def accuracy(self, x, t):\n",
    "        \n",
    "        #### 채워 넣으세요 ####\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return  accuracy\n",
    "    \n",
    "    \n",
    "    # 기울기 함수\n",
    "    # 수치 미분법으로 계산\n",
    "    # x : 입력 데이터\n",
    "    # t : 정답 레이블\n",
    "    def gradient(self, x, t):\n",
    "        # get loss function\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] =  numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] =  numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] =  numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] =  numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] =  numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] =  numerical_gradient(loss_W, self.params['b3'])\n",
    "        \n",
    "        #### 채워 넣으세요 ####\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba400582",
   "metadata": {},
   "source": [
    "## train, test - MNIST\n",
    "\n",
    "교재 github에서 /dataset/mnist.py 다운로드 후 load_mnist import해서 사용\n",
    "\n",
    "**ThreeLayerNet으로 train, test**\n",
    "\n",
    "- input_size = 784\n",
    "- hidden_size_1 = 50\n",
    "- hidden_size_2 = 100\n",
    "- output_size = 10\n",
    "- batch_size = 100\n",
    "- learning_rate = 0.1\n",
    "- iters_num = 5000\n",
    "- train_size = x_train data 전체\n",
    "\n",
    "**train loss 그래프 그리기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4331635",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m t_train[i\u001b[38;5;241m%\u001b[39mtrain_size:i\u001b[38;5;241m%\u001b[39mtrain_size\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#### 채워 넣으세요 ####\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# gradient 계산 후 parameter update 필요\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     37\u001b[0m     networkd\u001b[38;5;241m.\u001b[39mparams[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad[i]\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mThreeLayerNet.gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     72\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x,t)\n\u001b[1;32m     74\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 75\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     77\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m f(x)\n\u001b[1;32m     20\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tmp_val) \u001b[38;5;241m-\u001b[39m h\n\u001b[0;32m---> 21\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m grad[idx] \u001b[38;5;241m=\u001b[39m (fxh1 \u001b[38;5;241m-\u001b[39m fxh2) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m h)\n\u001b[1;32m     25\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mThreeLayerNet.gradient.<locals>.<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# get loss function\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     75\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mThreeLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     42\u001b[0m     \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#### 채워 넣으세요 ####\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  cross_entropy_error(y, t)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mThreeLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m W1, W2, W3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m b1, b2, b3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     28\u001b[0m z1 \u001b[38;5;241m=\u001b[39m sigmoid(a1)\n\u001b[1;32m     29\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "systrain_acc_listppend(os.pardir)\n",
    "\n",
    "from mnist import load_mnist\n",
    "\n",
    "# load dataset\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# Hyperparameters\n",
    "#### 채워 넣으세요 ####\n",
    "iters_num = 5000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = ThreeLayerNet(784, 50, 100, 10)\n",
    "\n",
    "# 1 epoch 되는 iteration\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "# Train\n",
    "for i in range(iters_num):\n",
    "    # get minibatch\n",
    "    # not random\n",
    "    x_batch = x_train[i%train_size:i%train_size+batch_size]\n",
    "    t_batch = t_train[i%train_size:i%train_size+batch_size]\n",
    "    \n",
    "    #### 채워 넣으세요 ####\n",
    "    # gradient 계산 후 parameter update 필요\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for i in ('W1', 'b1', 'W2', 'b2', 'W3', 'b1'):\n",
    "        networkd.params[i] -= learning_rate * grad[i]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    \n",
    "    # 1 epoch 당 accuracy 계산\n",
    "    if i%iter_per_epoch == 0:\n",
    "        #### 채워 넣으세요 ####\n",
    "        # train accuracy, test accuracy 각각 계산 후 (모델에서 정의한 accuracy 함수 이용)\n",
    "        # train_acc_list, test_acc_list에 각각 accuracy append\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        \n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "        \n",
    "        print(\"Train Accuracy: \" + str(train_acc) + \" & \" + \"Test Accuracy: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995fefa",
   "metadata": {},
   "source": [
    "### Visualize Loss\n",
    "- loss 보여주는 plot\n",
    "\n",
    "- 맘대로 시각화하면 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label = 'trainacc')\n",
    "plt.plot(x, test_acc_list, label = 'test acc', linestyle = '--')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94ebc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65473f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
